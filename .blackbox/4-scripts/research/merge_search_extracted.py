#!/usr/bin/env python3
from __future__ import annotations

import argparse
import json
from datetime import datetime, timezone
from pathlib import Path
from typing import Any


def merge_payloads(payloads: list[dict[str, Any]]) -> dict[str, Any]:
    repos_by_full: dict[str, dict[str, Any]] = {}
    queries: list[str] = []

    for p in payloads:
        for q in (p.get("queries_used") or []):
            if isinstance(q, str):
                queries.append(q)
        for r in (p.get("repos") or []):
            if not isinstance(r, dict):
                continue
            full = str(r.get("full_name") or "").strip()
            if not full:
                continue
            prev = repos_by_full.get(full)
            if not prev:
                repos_by_full[full] = r
                continue
            # Keep the “better” row (prefer higher stars; tie-break higher integration_score if present).
            prev_stars = int(prev.get("stars") or 0)
            new_stars = int(r.get("stars") or 0)
            if new_stars > prev_stars:
                repos_by_full[full] = r
                continue
            if new_stars == prev_stars:
                prev_is = int(prev.get("integration_score") or 0)
                new_is = int(r.get("integration_score") or 0)
                if new_is > prev_is:
                    repos_by_full[full] = r

    repos = list(repos_by_full.values())
    repos.sort(key=lambda r: (int(r.get("integration_score") or 0), int(r.get("stars") or 0)), reverse=True)

    # De-dupe queries while preserving order
    seen_q: set[str] = set()
    queries_out: list[str] = []
    for q in queries:
        key = q.strip()
        if not key:
            continue
        lk = key.lower()
        if lk in seen_q:
            continue
        seen_q.add(lk)
        queries_out.append(key)

    return {
        "generated_at_utc": datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ"),
        "count": len(repos),
        "queries_used": queries_out,
        "repos": repos,
    }


def main() -> int:
    ap = argparse.ArgumentParser(description="Merge github_search_repos.py --out-search-json dumps.")
    ap.add_argument("--in", dest="in_paths", action="append", required=True, help="Input search-extracted.json (repeatable).")
    ap.add_argument("--out", required=True, help="Write merged search-extracted.json here.")
    ap.add_argument("--out-repos", default="", help="Optional: write owner/repo list here (sorted).")
    ap.add_argument("--limit", type=int, default=0, help="Limit repo list (0 = all).")
    args = ap.parse_args()

    payloads: list[dict[str, Any]] = []
    for p in args.in_paths:
        payloads.append(json.loads(Path(p).read_text("utf-8")))

    merged = merge_payloads(payloads)
    if args.limit and args.limit > 0:
        merged["repos"] = (merged.get("repos") or [])[: args.limit]
        merged["count"] = len(merged["repos"])

    out = Path(args.out)
    out.parent.mkdir(parents=True, exist_ok=True)
    out.write_text(json.dumps(merged, indent=2, sort_keys=True) + "\n", encoding="utf-8")
    print(f"Wrote merged search JSON: {out}")

    if args.out_repos.strip():
        out_repos = Path(args.out_repos)
        out_repos.parent.mkdir(parents=True, exist_ok=True)
        lines: list[str] = []
        lines.append(f"# Generated by merge_search_extracted.py at {merged['generated_at_utc']}")
        for r in (merged.get("repos") or []):
            full = str(r.get("full_name") or "").strip()
            if full:
                lines.append(full)
        out_repos.write_text("\n".join(lines) + "\n", encoding="utf-8")
        print(f"Wrote merged repo list: {out_repos}")

    return 0


if __name__ == "__main__":
    raise SystemExit(main())

