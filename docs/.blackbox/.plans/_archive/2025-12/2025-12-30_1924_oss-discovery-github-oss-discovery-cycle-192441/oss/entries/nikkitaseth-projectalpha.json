{
  "id": 277983552,
  "node_id": "MDEwOlJlcG9zaXRvcnkyNzc5ODM1NTI=",
  "name": "ProjectAlpha",
  "full_name": "Nikkitaseth/ProjectAlpha",
  "private": false,
  "owner": {
    "login": "Nikkitaseth",
    "id": 60305765,
    "node_id": "MDQ6VXNlcjYwMzA1NzY1",
    "avatar_url": "https://avatars.githubusercontent.com/u/60305765?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/Nikkitaseth",
    "html_url": "https://github.com/Nikkitaseth",
    "followers_url": "https://api.github.com/users/Nikkitaseth/followers",
    "following_url": "https://api.github.com/users/Nikkitaseth/following{/other_user}",
    "gists_url": "https://api.github.com/users/Nikkitaseth/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/Nikkitaseth/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/Nikkitaseth/subscriptions",
    "organizations_url": "https://api.github.com/users/Nikkitaseth/orgs",
    "repos_url": "https://api.github.com/users/Nikkitaseth/repos",
    "events_url": "https://api.github.com/users/Nikkitaseth/events{/privacy}",
    "received_events_url": "https://api.github.com/users/Nikkitaseth/received_events",
    "type": "User",
    "user_view_type": "public",
    "site_admin": false
  },
  "html_url": "https://github.com/Nikkitaseth/ProjectAlpha",
  "description": "PYTHON CODE WALKTHROUGH Data Sourcing In order to run a discounted cash flow model (DCF), I needed data, so I found a free API that provided us with everything I needed. I wrote a code that saved every financial statement of every company in a separate text file. In this code, I asked to ping the API\u2019s URL for every ticker, open a text file for one of the financial statements for one company ticker, dump all the data found by the code into this file, and close it. This process was repeated for every company in our company list and every statement I have a code for. By doing so I Ire able to store the data for every company locally and did not need to ping the API every time I ran our code. Once all the financial data for each company was stored in form of a balance sheet, income statement, cash flow statement, and company profile text file, I needed to pick out specific items required for our DCF model. Thus, I defined the functions that selected all required items from the respective financial statements of each company and assigned them to a variable using utils.py.  Discounted Cash Flow Model First of all, I needed to import the functions I defined in utils.py before defining the DCF model function, which would run for every company in our list. Next, I ensured to have 5 consecutive years of past data to compute the average. Thus, the first few lines of code checked whether the last year on record was 2019 from which point I would go back 5 years; if the last year was 2018, this would be taken as the first data entry from which I would go back 5 years. The second part mentioned above is important because companies file their 10-K, i.e. their annual report, at different times throughout the year so there may be companies that already filed their reports while others had not. After this step, five-year averages of every item\u2019s percentage of revenue Ire calculated as Ill as the average revenue growth over the same period. These items included EBIT, depreciation & amortization, capital expenditures, and the change in net working capital. Once that was done, there Ire only three variables missing before calculating free cash flows for the next few years: a discount or hurdle rate; industry-specific perpetual growth rates; and a tax rate. After these three variables Ire set up, the next step was to calculate the free cash flows to the firm (fcff) for the next 5 years and determine the terminal value at the end of the period using the growth rate for the corresponding industry. For the former, I use a loop to calculate the fcff for all the year, discount it, and add it to one variable called fcffpv. Once the terminal value was calculated, these two additional numbers captured the enterprise value of the firm. Since I Ire interested in the equity value, I subtracted debt and add cash, which left us with the equity value. In one final step, I divided this value by the number of shares to end up with an intrinsic value per share. After calculating the intrinsic value per share, I compared it to the current share price with two additions. First, I added a buffer to minimize our downside risk for inaccuracy in calculations, which is called the margin of safety. Here, the intrinsic value should at least be 115% of the current share price. I also set an upper limit at 130% to ensure I would not include companies with extraordinarily high valuations, compared to their current price. If the share price calculated fell within this window, I added its ticker to a dataframe, which was the last step in the function. As such, the DCF function would run for every company and provide a dataframe with the tickers of all those companies that Ire undervalued at the time and fell within the 115% - 130% range.  Portfolio Optimization The dataframe with the tickers of all the undervalued companies that was previously created has now become the portfolio, which I converted into a list and used as the source for further optimization that is about to come. Some general inputs for the rest of the code Ire the start and end date of the data I requested for optimization, as Ill as the risk-free rate and the number of simulations I wanted to run our optimizations for. Now that the general framework has been created, it is time to choose some conditioning variables to measure the performance of investment in one sector or across a combination of some/all sectors, respectively.  Project Alpha uses the following conditioning variables to optimize its portfolios: \u2022 Sharpe Ratio: It measures the performance of an investment compared to the risk-free asset, i.e. the 10-year Treasury Bond, after adjusting for its risk factor or standard deviation. The Sharpe ratio would be given a higher Iight for investors who have a higher risk tolerance.  In terms of code, I used the bt package to retrieve the data betIen the predetermined start and end date for the companies in our ticker list. This data was then used to find the portfolio with the highest Sharpe ratio. For that, random Iights Ire assigned to each company and the ratio was computed. After running the number of simulations previously determined, the Iights with the highest Sharpe ratio will be located using loc() and labeled \u2018sharpe_portfolio\u2019 which is a dataframe containing the excess return, the volatility, Sharpe ratio, as Ill as the Iights for every company. I also located the portfolio with the loIst volatility, put it in a dataframe called \u2018min_volatility_port\u2019 which has the same attributes. The rest of the code of this segment simply created a picture with all the portfolios generated, displaying the efficient frontier and highlighting the portfolio with the highest Sharpe ratio and loIst volatility. \u2022 Value at Risk (VaR): VaR was chosen as a diagnostic tool to assess the model. In our case, it basically indicated the percentage of time in which a loss greater than 1% would occur over a period of 5 years. Its limitation is that although it measures how bad the best of the bad is, it does not measure how bad it can get, meaning the worst of the worst. In regards to the code, I first requested the adjusted closing for the companies in our ticker list in the determined time horizon. I then retrieved the Iights from our Sharpe portfolio, set the number of days I wanted to simulate as Ill as the cutoff, before calculating the returns of every company in every period; here: daily. Thereafter, I created a new variable called \u2018sigma\u2019, which was be a copy of our return variable, in order to ensure the right format and type for our Monte Carlo loop. The simulation is pretty straight forward, as it measures how many runs the returns fall within 1% or outside of it. I then Iighed the resulting returns by the Iight of the company in the portfolio and whenever the portfolio return was outside the set boundary, it would count as a \u2018bad simulation\u2019. Once that is done, the number of bad simulations was divided by the total number of simulations to end up with a percentage of how many simulations were bad, which equals our VaR \u2022 Treynor Ratio: For the investors that already have a perfectly diversified portfolio and would like to add more assets to it, there would be a higher Iight on the Treynor ratio. It basically uses beta as a risk factor because it carries the risk relative to the market, instead of standard deviation as in Sharpe, meaning only systematic or non-diversifiable risk.  For the code, I first calculated the portfolio\u2019s beta. For that, I defined a function \u2018beta\u2019 that reads the beta of every company and returns it. The next step is to run a loop that would enter the beta of every company in our ticker list into a new dataframe. After setting the index equal to the tickers and transposing the Sharpe portfolio Iights, I can concat the two thus resulting in two columns: one is the beta of every company and the second is the corresponding Iight in the portfolio. I then created a third column as the product of columns one and two. The sum of all entries in that column is the portfolio beta, which was then used as the denominator for the ratio. The nominator was already calculated as \u2018Excess Return\u2019 in the Sharpe portfolio. \u2022 Sortino Ratio: The Sortino ratio measures only the downside risk (downside deviation or semi-deviation) by measuring returns against a minimum acceptable return, \ud835\udf0f. It is surprising to know that most of the industry ignores the total number of periods taken and just calculates the downside deviation by choosing the periods with downside risk, which results in misleading results. Project Alpha uses all the periods to calculate the same, so as to have an advantage over those robo-advisors/financial advisors that do not follow this process. The alpha in the future would be generated by going long on companies with high correct Sortino and low incorrect Sortino as they are undervalued, and shorting those with low correct Sortino and high incorrect Sortino as these are overvalued. The Sortino ratio would be given more Iight for investors who are more risk averse.  This part of the code started with retrieving the data for our benchmark, the S&P 500, for the period and the calculating the average daily and annual return. After that, I calculate the portfolio returns, \u2018returns[\u201cReturns\u201d]\u2019, by adding the products of every company\u2019s Iight times its return, which gave us the portfolio return for every period. From here, I calculated the downside risk by comparing the portfolio return in every period to the daily average return of our benchmark in a for loop. Before I did that, I defined a new variable called \u2018semi\u2019, which is a data series and will be filled with whatever comes out of the loop every single time. If the portfolio return minus the average daily return of the benchmark was greater than 0 \u2013 meaning the portfolio earned more than the average of the S&P500 \u2013 the value for the period was set to 0 and added to the semi data series. If it is 0, which is extremely unlikely, but whatever, it would also be 0. If it is less than 0, hoIver, which indicates underperformance, I would square the portfolio return, which already gives us the semi variance I need for our next step. From here, I can simply take the square root of the average of the \u2018semi\u2019 data series to get the daily downside risk and multiplying it by the square root of 252, which gives us the annual number. After that, I have all the numbers to calculate the Sortino ratio. \u2022 Information Ratio: The information ratio measures the portfolio returns compared to the returns of a benchmark index, i.e. S&P500, after adjusting for its additional risk. It only looks at the excess return of the portfolio over the benchmark and the volatility or risk associated with it. I already have all the inputs I need to calculate his ratio. Thus, I simply created a new dataframe with the portfolio returns of every period and the benchmark returns of every period. To find the excess return, i.e. the nominator, I simply subtracted the latter from the former and assigned it to a new variable, which I called \u2018excess_return\u2019. The nominator would be the average return of the portfolio minus the average return of the benchmark, and the denominator would be the standard deviation of the \u2018excess_return\u2019 series. Finally, I printed short sentences with the results for every conditioning variable just described as an output in the console.",
  "fork": false,
  "url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha",
  "forks_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/forks",
  "keys_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/keys{/key_id}",
  "collaborators_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/collaborators{/collaborator}",
  "teams_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/teams",
  "hooks_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/hooks",
  "issue_events_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/issues/events{/number}",
  "events_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/events",
  "assignees_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/assignees{/user}",
  "branches_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/branches{/branch}",
  "tags_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/tags",
  "blobs_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/git/blobs{/sha}",
  "git_tags_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/git/tags{/sha}",
  "git_refs_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/git/refs{/sha}",
  "trees_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/git/trees{/sha}",
  "statuses_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/statuses/{sha}",
  "languages_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/languages",
  "stargazers_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/stargazers",
  "contributors_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/contributors",
  "subscribers_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/subscribers",
  "subscription_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/subscription",
  "commits_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/commits{/sha}",
  "git_commits_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/git/commits{/sha}",
  "comments_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/comments{/number}",
  "issue_comment_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/issues/comments{/number}",
  "contents_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/contents/{+path}",
  "compare_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/compare/{base}...{head}",
  "merges_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/merges",
  "archive_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/{archive_format}{/ref}",
  "downloads_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/downloads",
  "issues_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/issues{/number}",
  "pulls_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/pulls{/number}",
  "milestones_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/milestones{/number}",
  "notifications_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/notifications{?since,all,participating}",
  "labels_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/labels{/name}",
  "releases_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/releases{/id}",
  "deployments_url": "https://api.github.com/repos/Nikkitaseth/ProjectAlpha/deployments",
  "created_at": "2020-07-08T03:48:32Z",
  "updated_at": "2025-10-21T10:03:43Z",
  "pushed_at": "2020-07-08T03:52:51Z",
  "git_url": "git://github.com/Nikkitaseth/ProjectAlpha.git",
  "ssh_url": "git@github.com:Nikkitaseth/ProjectAlpha.git",
  "clone_url": "https://github.com/Nikkitaseth/ProjectAlpha.git",
  "svn_url": "https://github.com/Nikkitaseth/ProjectAlpha",
  "homepage": null,
  "size": 15,
  "stargazers_count": 22,
  "watchers_count": 22,
  "language": "Python",
  "has_issues": true,
  "has_projects": true,
  "has_downloads": true,
  "has_wiki": true,
  "has_pages": true,
  "has_discussions": false,
  "forks_count": 8,
  "mirror_url": null,
  "archived": false,
  "disabled": false,
  "open_issues_count": 0,
  "license": null,
  "allow_forking": true,
  "is_template": false,
  "web_commit_signoff_required": false,
  "topics": [],
  "visibility": "public",
  "forks": 8,
  "open_issues": 0,
  "watchers": 22,
  "default_branch": "master",
  "permissions": {
    "admin": false,
    "maintain": false,
    "push": false,
    "triage": false,
    "pull": true
  },
  "temp_clone_token": "",
  "network_count": 8,
  "subscribers_count": 3
}